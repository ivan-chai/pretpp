module:
  freeze_prefixes:
    - _seq_encoder.model.model.embeddings.word_embeddings.
    - _seq_encoder.model.model.pooler.
  seq_encoder:
    _target_: hotpp.nn.Encoder
    model_partial:
      _target_: hotpp.nn.HuggingFaceTransformer
      _partial_: true
      model:
        _target_: transformers.RobertaModel
        config:
          _target_: transformers.RobertaConfig
          vocab_size: 2
          n_embd: ${transformer_hidden_size}
          intermediate_size: ${transformer_inter_size}
          max_position_embeddings: ${transformer_positions}
          hidden_size: ${transformer_hidden_size}
          num_hidden_layers: ${transformer_layers}
          num_attention_heads: ${transformer_heads}
          output_hidden_states: True  #Necessary argument
    max_context: ${transformer_context}
