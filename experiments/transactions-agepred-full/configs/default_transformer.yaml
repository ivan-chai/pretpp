defaults:
  - _self_

module:
  seq_encoder:
    _target_: hotpp.nn.Encoder
    model_partial:
      _target_: hotpp.nn.HuggingFaceTransformer
      _partial_: true
      model:
        _target_: transformers.RobertaModel
        config:
          _target_: transformers.RobertaConfig
          vocab_size: 1
          n_embd: ${transformer_hidden_size}
          max_position_embeddings: ${transformer_positions}
          hidden_size: ${transformer_hidden_size}
          num_hidden_layers: ${transformer_layers}
          num_attention_heads: ${transformer_heads}
          output_hidden_states: True  #Necessary argument
    max_context: ${transformer_context}
