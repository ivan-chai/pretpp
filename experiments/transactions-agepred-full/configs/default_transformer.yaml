module:
  freeze_prefixes:
    - _seq_encoder.model.model.wte.
  seq_encoder:
    _target_: hotpp.nn.Encoder
    model_partial:
      _target_: hotpp.nn.HuggingFaceTransformer
      _partial_: true
      model:
        _target_: transformers.GPT2Model
        config:
          _target_: transformers.GPT2Config
          vocab_size: 1
          n_positions: ${transformer_positions}
          n_embd: ${transformer_hidden_size}
          n_layer: ${transformer_layers}
          n_head: ${transformer_heads}
          n_inner: ${transformer_inter_size}
          output_hidden_states: True  #Necessary argument
    max_context: ${transformer_context}
