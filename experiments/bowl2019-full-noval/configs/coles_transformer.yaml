defaults:
  - default
  - default_transformer
  - _self_

name: coles_transformer

module:
  _target_: pretpp.modules.BaseModule
  seq_encoder:
    rnn_partial:
      hidden_size: 1024
  optimizer_partial:
    lr: 0.004
  lr_scheduler_partial:
    gamma: 0.9025
    step_size: 10
  head_partial:
    _target_: pretpp.nn.NormalizationHead
    _partial_: true
  loss_projection_partial: ${nohead}
  loss:
    _target_: pretpp.losses.ColesLoss
    embedding_dim: ${module.seq_encoder.rnn_partial.hidden_size}
    id_field: user_id
    n_splits: 5
    coles_loss:
      _target_: ptls.frames.coles.losses.ContrastiveLoss
      margin: 0.5
      sampling_strategy:
        _target_: ptls.frames.coles.sampling_strategies.HardNegativePairSelector
        neg_count: 5
    cls_token: ${transformer_cls_token}
trainer:
  max_epochs: 120
  check_val_every_n_epoch: 10
