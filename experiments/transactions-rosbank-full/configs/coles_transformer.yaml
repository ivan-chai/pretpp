defaults:
  - default
  - default_transformer
  - coles_loss@module.loss
  - _self_

name: coles_transformer

module:
  _target_: pretpp.modules.BaseModule
  optimizer_partial:
    lr: 0.004
  lr_scheduler_partial:
    gamma: 0.9025
    step_size: 10
  head_partial:
    _target_: pretpp.nn.NormalizationHead
    _partial_: true
  aggregator:
    _target_: hotpp.nn.LastAggregator
  loss_projection_partial: ${nohead}
  loss:
    embedding_dim: ${transformer_hidden_size}
trainer:
  max_epochs: 120
  check_val_every_n_epoch: 10
