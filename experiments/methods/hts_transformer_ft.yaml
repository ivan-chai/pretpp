defaults:
  - default
  - default_hts_transformer
  - default_ft
  - _self_

base_name: _

module:
  seq_encoder:
    model_partial:
      strategy_partial:
        apply_probability: 0  # Insert HTs, but don't use them for regular tokens.
        predict: all  # Output both HT and regular tokens. SFT is applied to all outputs.
#  aggregator:
#    _target_: hotpp.nn.LastAggregator
#  head_partial:
#    _target_: pretpp.nn.NormalizationHead
#    _partial_: true
#  seq_encoder:
#    model_partial:
#      dropout: 0.3
#  loss:
#    cls_token: ${transformer_cls_token}
#  loss:
#    apply_to_all_outputs: false
  peft_adapter:
    _partial_: true
    _target_: peft.get_peft_model
    peft_config:
      _target_: peft.LoraConfig
      task_type:
        _target_: peft.TaskType
        _args_: ["SEQ_CLS"]
      r: 16
      lora_alpha: 32
      lora_dropout: 0.2
      target_modules: ["self_attn", "linear1", "linear2"]
#  freeze_prefixes:
#    - _seq_encoder.embedder.
#    - _seq_encoder.model.encoder.layers.1
#    - _seq_encoder.model.encoder.layers.2
